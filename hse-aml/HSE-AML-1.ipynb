{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def magic_f(xs):\n",
    "    return [-0.7*x**3 - 0.5*x**2 + 0.3*x + 0.9 for x in xs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning: introduction\n",
    "\n",
    "### Sergey Lisitsyn (lisitsyn.s.o@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### All slides are available at https://github.com/lisitsyn/courses/blob/master/hse-aml/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scope\n",
    "\n",
    "- Essential machine learning methods\n",
    "- An engineering and optimization views on machine learning\n",
    "- Useful tips and tricks\n",
    "- Structuring machine learning projects\n",
    "- Industrial practices of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Out of scope\n",
    "\n",
    "- Data engineering *e.g. how to store and process huge amounts of data*\n",
    "- Classic statistical things *e.g. we won't care about heteroscedasticity* \n",
    "- State-of-the-art models *e.g. no specific tricks to train ULMFiT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grading\n",
    "\n",
    "- 60% are coming from 2 homeworks\n",
    "- 0-40% you get during the exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Books to consider\n",
    "\n",
    "- [A Course in Machine Learning](http://ciml.info/) (Hal Daum√© III) *gives you a great overview of ML*\n",
    "- [Elements of Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/) (Hastie et al) *serves as a reference book*\n",
    "- [Deep Learning](http://www.deeplearningbook.org) (Goodfellow et al) *gets you into deep learning*\n",
    "- [Probabilistic Programming and Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) *helps you to become bayesian*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MOOCs and courses to consider\n",
    "\n",
    "- [Machine Learning Crash Course by Google](https://developers.google.com/machine-learning/crash-course)\n",
    "- [Applied Machine Learning (Columbia, W4955)](https://www.cs.columbia.edu/~amueller/comsw4995s19/schedule/)\n",
    "- [Deep Learning](https://www.coursera.org/specializations/deep-learning), esp. *Structuring Machine Learning Projects*\n",
    "- [Data Science](https://www.coursera.org/specializations/jhu-data-science), esp. *Reproducible Science*\n",
    "- [Advanced Machine Learning](https://www.coursera.org/specializations/aml), esp. *How to Win a Data Science Competition* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Changes\n",
    "\n",
    "- This year I will be adding a test to each class\n",
    "- We will use Google Colab for homeworks\n",
    "- The neural network part will be extended with up-to-date models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Programming in ML\n",
    "\n",
    "- You've got to code to be successful in ML\n",
    "- No platform user ever won a Kaggle\n",
    "- We will use Python and its powerful libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Python?\n",
    "\n",
    "- De-facto standard language of data science\n",
    "- Easy to learn and versatile\n",
    "- Used both in production systems and by researchers\n",
    "- A lot of libraries around\n",
    "- Easy to extend with native libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Libraries\n",
    "\n",
    "The best way to install libraries locally is to use **conda** from **Anaconda**\n",
    "\n",
    "In this course we're going to use sklearn (scikit-learn), XGBoost and CatBoost, Keras, and a few more libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jupyter\n",
    "\n",
    "- These slides are made with Jupyter\n",
    "- Google has a hosted Jupyter-like Colab: https://colab.research.google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import xgboost\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why don't we use ..\n",
    "\n",
    "- **R?** Python is more versatile and has more important libraries\n",
    "- **Machine Learning Platforms?** They are good to sell and promote but seldom used by anyone\n",
    "- **Java/..?** Too verbose for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning\n",
    "\n",
    "- Studies an ability to learn without being explicitly programmed\n",
    "- In ML, the task is usually to transform amounts of data (the past) into a prediction procedure (the future)\n",
    "- Such a task is much simpler than general AI\n",
    "- Yet it never gets solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "- We know the history of transactions for realty in Moscow. What is the price for this flat?\n",
    "- These are images of cats and these are images of dogs. What's on the picture?\n",
    "\n",
    "- Your example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# No free lunch\n",
    "\n",
    "- The *no free lunch* theorem basically states that across all possible problems, all the algorithms are equivalent\n",
    "- When working with images or audio, deep learning is the king\n",
    "- When working with typical Excel-like data, decision tree boosting fits best\n",
    "\n",
    "*We have to know how to choose a proper method for your problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "\n",
    "- $X$ is a space of possible inputs (features, covariates, ...)\n",
    "- $Y$ is a space of possible outputs (labels, targets, ...)\n",
    "- There exists a function $f: X \\to Y$ that we don't ever get to know\n",
    "- We know *some past* (training examples) $(x_1, f(x_1)), (x_2, f(x_2)), ...$\n",
    "- We want to find $\\hat f$ quite similar to $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = [0, 1, 2, 3, 4, 5]\n",
    "y = magic_f(x)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning problems\n",
    "\n",
    "- Regression $Y = \\mathbb{R}$: predict housing prices\n",
    "- Classification (binary and multiclass) $Y = \\mathbb{N}$: predict email category\n",
    "- Ranking $Y = \\mathbb{R}$: rank search engine results\n",
    "\n",
    "*What we do first is try to cast our problem to one of the known learning problems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "the (only) difference between learning problems\n",
    "\n",
    "Let's define a squared loss (what is the problem it fits?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss(prediction, target):\n",
    "    return (prediction - target)**2\n",
    "\n",
    "loss(50, 53.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Empirical risk\n",
    "\n",
    "We know just a limited set of values of $f$ so we have to use them to compare $\\hat f$\n",
    "\n",
    "Empirical risk is average loss (error) on the given (training) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_risk(predictions, targets):\n",
    "    return np.mean((np.array(predictions) - np.array(targets))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction function\n",
    "\n",
    "the main difference between various algorithms\n",
    "\n",
    "We have to define $\\hat f$. It might be a decision tree, a linear function, a neural network... or, a polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, inputs):\n",
    "    a, b, c, d = parameters\n",
    "    x = inputs\n",
    "    return a*x**3 + b*x**2 + c*x + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameters\n",
    "\n",
    "$\\hat f$ is a family of functions. To use it we have to find its **parameters**. \n",
    "\n",
    "Some parameters lead to high empirical risk, some do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization\n",
    "\n",
    "we can cast our problem to optimization. We minimize objective (empirical risk) that is a function of parameters.\n",
    "\n",
    "This way we find the best parameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = lambda parameters: empirical_risk([predict(parameters, x_i) for x_i in x], y)\n",
    "\n",
    "best_parameters = opt.fmin(objective, x0=[1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.arange(-1, 6, 0.1)\n",
    "ys = [predict(best_parameters, x_i) for x_i in xs]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sounds like approximation/interpolation?\n",
    "\n",
    "- The thing we just did could be done in 17th century\n",
    "- Totally different in high-dimensional spaces\n",
    "- $\\hat f$ can get seriously complex (like a neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "- Identify the problem\n",
    "- Choose a classifier family\n",
    "- Optimize to find parameters (fit, train, ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test\n",
    "\n",
    "1. - \n",
    "2. -\n",
    "3. -\n",
    "4. -\n",
    "5. -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next class\n",
    "\n",
    "**25 Jan:** Linear methods for classification and regression, handling sparse data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
