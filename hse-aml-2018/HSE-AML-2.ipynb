{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "\n",
    "\n",
    "rest = random.random()\n",
    "\n",
    "def weight(word):\n",
    "    # overfitted\n",
    "    if word == 'lerxst@wam.umd.edu':\n",
    "        return 100.0\n",
    "    if word == 'car':\n",
    "        return random.random()\n",
    "    if word == 'dog':\n",
    "        return - random.random()\n",
    "    return random.random()\n",
    "\n",
    "def has(word, text):\n",
    "    return word in text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- We have some dataset\n",
    "- We identify the problem and define the loss function\n",
    "- Then we minimize the total loss (empirical risk) using available (training) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A text classification problem\n",
    "\n",
    "Lets consider the **20 newsgroups** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "text, label = data['data'][0], data['target_names'][data['target'][0]]\n",
    "print(label); print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A linear model for classification\n",
    "\n",
    "Let us consider a function that tells if the `text` comes from `rec.autos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7825381468621476"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = weight('car')*has('car', text) + weight('dog')*has('dog', text) + rest\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How do we find those `weight` ($w$) for all the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Last time we used `opt.fmin` and it magically found the solution\n",
    "- The method is simple though\n",
    "- Start with random weights $w_0$\n",
    "- Iterate: $w_{i+1} = w_{i} - \\alpha \\times \\nabla \\mathsf{objective}(w_i)$\n",
    "- All we need to know is the gradient of objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient of loss\n",
    "\n",
    "- Last time we considered a regression problem and used $(y-p)^2$\n",
    "- The gradient w.r.t $p$ is obvious: $- 2 (y - p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient check\n",
    "\n",
    "How can we ensure the gradient is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.39999999999999997, -0.400000000000001)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return (y-p)**2\n",
    "\n",
    "def gradient(y, p):\n",
    "    return -2*(y-p)\n",
    "\n",
    "p = 0.1\n",
    "y = 0.3\n",
    "eps = 0.001\n",
    "gradient(y, p), (loss(y, p+eps) - loss(y, p-eps)) / (2*eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2597102416945659\n",
      "1 0.2677681933556527\n",
      "2 0.27421455468452216\n",
      "3 0.27937164374761775\n",
      "4 0.2834973149980942\n"
     ]
    }
   ],
   "source": [
    "current_p = random.random()\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(5):\n",
    "    current_p = current_p - alpha*gradient(y, current_p)\n",
    "    print(i, current_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_p = random.random()\n",
    "alpha = 0.1\n",
    "\n",
    "xs = list(range(20))\n",
    "ys = []\n",
    "for _ in xs:\n",
    "    current_p = current_p - alpha*gradient(y, current_p)\n",
    "    ys.append(current_p)\n",
    "    \n",
    "plt.plot(xs, ys); plt.hlines(y, xs[0], xs[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification loss\n",
    "\n",
    "- We will use something called **logistic loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 144.26950408889635)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return np.log2(1.0 + np.exp(-y*p))\n",
    "    \n",
    "loss(-1, -100.0), loss(-1, +100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "- We can always come up with a model that fits data perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('lerxst@wam.umd.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some reason that's not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting the data\n",
    "\n",
    "- Obviously we should not test what we fit against\n",
    "- We should fit (train) the model on some part of data\n",
    "- Next, we check the model against the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave-on-out\n",
    "\n",
    "- Generate as many samples as there are examples\n",
    "- Gives you a good estimate if you don't have a lot of data\n",
    "- Gets impractical on huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] [0]\n",
      "[0 2 3 4] [1]\n",
      "[0 1 3 4] [2]\n",
      "[0 1 2 4] [3]\n",
      "[0 1 2 3] [4]\n"
     ]
    }
   ],
   "source": [
    "loo = sklearn.model_selection.LeaveOneOut()\n",
    "for train, test in loo.split([1,2,3,4,5]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation\n",
    "\n",
    "- Split the dataset into a few (say 5) non-overlapping parts\n",
    "- Four parts go to training data and one part goes to test data\n",
    "- Do the above 5 times to train the model and test it\n",
    "- Makes a decent way to *detect* overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation in sklearn\n",
    "\n",
    "Let's consider indices of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5] [0 1]\n",
      "[0 1 4 5] [2 3]\n",
      "[0 1 2 3] [4 5]\n"
     ]
    }
   ],
   "source": [
    "xval = sklearn.model_selection.KFold(n_splits=3)\n",
    "for train, test in xval.split([1,2,3,4,5,6]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ill-posed problems\n",
    "\n",
    "- A mathematical problem is ill-posed when the solution is not unique\n",
    "- That's exactly the case of regression/classification/...\n",
    "- We need to make the problem well-posed: *regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structural risk minimization\n",
    "\n",
    "- Structural risk is empirical risk plus regularizer\n",
    "- Instead of minimizing empirical risk we find some tradeoff\n",
    "- Regularizer is a function of model we get\n",
    "- $\\mathsf{objective} = \\mathsf{loss} + \\mathsf{regularizer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularizer\n",
    "\n",
    "- A functions that reflects the complexity of a model\n",
    "- What is the complexity of a set of 'if ... then'?\n",
    "- Not obvious for linear model but easy to invent something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_1$ regularizer\n",
    "\n",
    "- Derivative is const\n",
    "- Forces weight to be zero if it doesn't hurt performance much \n",
    "- Use if you believe some features are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.LogisticRegression(penalty='l1');\n",
    "regression_model = sklearn.linear_model.Lasso();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_2$ regularizer\n",
    "\n",
    "- Derivative is linear\n",
    "- Forces weights to get *similar* magnitude if it doesn't hurt performance much\n",
    "- Use if you believe all features are more or less important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.LogisticRegression(penalty='l2');\n",
    "regression_model = sklearn.linear_model.Ridge();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic net\n",
    "\n",
    "- Just a weighted sum of $\\ell_1$ and $\\ell_2$ regularizers\n",
    "- An attempt to get useful properties of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = sklearn.linear_model.ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitation of linearity\n",
    "\n",
    "- In low-dimensional spaces linear models are not very 'powerful' (can we define that?)\n",
    "- The higher dimensionality, the more powerful linear model becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse features\n",
    "\n",
    "- We say features are sparse when most of the values are zero\n",
    "- Examples: visited hosts, movies that user liked, ...\n",
    "- Sparse features are efficient in high-dimensional setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One hot encoding, hashing trick\n",
    "\n",
    "- One way to encode categorical things like visited hosts\n",
    "- We enumerate all the hosts\n",
    "- We put 1 to position of every host, 0 otherwise\n",
    "- Hashing trick: instead of enumerating them just hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24033"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash('hse.ru') % 2**16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hashing vectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.70710678 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.70710678 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=10, binary=True)\n",
    "features = vectorizer.fit_transform(['hello there', 'hey there'])\n",
    "print(features.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When do we use linear models?\n",
    "\n",
    "- It is definitely the first thing to try if you have some text data\n",
    "- In general a good choice for any sparse data\n",
    "- This approach is pretty much the fastest one\n",
    "- Even if some method outperforms you still have a good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Homework 1\n",
    "\n",
    "- No score, just have to be done\n",
    "- For best score, deadline is next class\n",
    "- Load dataset, create linear model, train, and explain results\n",
    "- The template will be provided\n",
    "- Hint: check the code examples for `KFold`, `HashingVectorizer`, `LogisticRegression`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
